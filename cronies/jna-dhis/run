#!/usr/bin/env python
# apt-get install libbz2-dev ssh-client postgresql-client
# pip install --upgrade pandas openpyxl SQLAlchemy psycopg2-binary google-api-python-client google-auth-httplib2 google-auth-oauthlib oauth2client
# cron_time 28 08 * * 4

import os, sys

sys.path.append("../libs")
import pandas as pd
import sqlalchemy
from src import dhis as dh
import utils as fn
import drive as gd
from src import query


def download_matview_data(postgres_url: str, month: str, e_map: pd.DataFrame):
    print("Now starting to download data from sql view...")
    with fn.run_cmd("ssh -4 -T medic-db") as shell:
        print(shell.results)
        with sqlalchemy.create_engine(postgres_url).connect() as con:
            for db_view in e_map.db_view.unique():
                print(".....downloading: ", db_view)
                data = pd.read_sql(sqlalchemy.text(query.get_sql(db_view, month)), con)
                data.to_csv(f".data/views/{db_view}-{month}.csv", index=False)
    e_map.to_csv(f".data/element_map-{month}.csv")


def change_columns_include_tablename(file_name, df):
    common = ["orgUnit", "categoryOptionCombo", "period"]
    db_view = file_name.split("-")[0]
    print(f'.... processing {db_view}')
    df.columns = [x if x in common else f"{db_view}__{x}" for x in df.columns]
    return df


def process_downloaded_data(dhis: object, month: str, e_map: pd.DataFrame):
    print("Starting to convert into DHIS2 payload ....")
    common = ["orgUnit", "categoryOptionCombo", "period"]
    data = pd.DataFrame(columns=common)
    files = filter(lambda x: month in x, os.listdir(".data/views"))
    for file in files:
        df = pd.read_csv(f".data/views/{file}")
        df["period"] = pd.to_datetime(df.reported_month).dt.strftime("%Y%m")
        df = dhis.add_category_combos_id(df)
        df = dhis.add_org_unit_id(df)
        df = df[df.orgUnit.notna()]
        df = change_columns_include_tablename(file,df)
        data = pd.merge( data, df, how="outer", on=common)
    data = data.drop(columns=[x for x in data.columns if "drop" in x])
    return dhis.to_data_values(data, e_map)


def upload(
    dhis: dh.DHIS,
    data: pd.DataFrame,
):
    print("Starting to upload payload...")
    res = dhis.Results()
    for ds_id in data.dataSet.unique():
        print("\n", ds_id)
        ds = data[data.dataSet == ds_id].drop("dataSet", axis=1).reset_index(drop=True)
        fn.do_chunks(
            source=ds.orgUnit.unique(),
            chunk_size=10,
            func=lambda orgs_names: dhis.upload_orgs(ds_id,orgs_names,data) ,
            consumer_func=lambda _, v: res.add(ds_id, v),
            thread_count=16,
        )
    return res


def get_the_mapping_file(conf: object):
    print("seeking mapping file from google drive ....")
    e_map = gd.Drive(conf.drive_key).get_df(conf.e_map_google, "data_elements")
    # e_map = pd.read_csv("./.data/element_map-2023-04-01.csv")
    e_map = e_map[e_map.db_column.notna() & e_map.element_id.notna()].copy()
    e_map["map_key"] = e_map.db_view + "__" + e_map.db_column
    return e_map.set_index('map_key')


def main(month=fn.get_month(-1)):
    print("loading the config")
    # conf = fn.get_config("/home/n2/Desktop/configs/jna-dhis-config.json")
    conf = fn.get_config()
    print(f"initiating connection dhis ... ")
    dhis = dh.DHIS(conf.dhis_url)
    e_map = get_the_mapping_file(conf)
    download_matview_data(conf.jna_postgres_url, month, e_map)
    data = process_downloaded_data(dhis, month, e_map)
    res = upload(dhis, data)
    print("Starting to refresh analytics ...")
    dhis.refresh_analytics()
    print("Now sending summary to slack")
    return res.send_to_slack(
        conf.slack_webhook_url, month, e_map[["dataset_id", "dataset_name"]]
    )


if __name__ == "__main__":
    x = sys.argv[1] if len(sys.argv) > 1 else fn.get_month(-1)
    data = main(month=x)
    print(data)
